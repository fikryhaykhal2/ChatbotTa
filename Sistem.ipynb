{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb59b6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.7.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.12.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.57.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (2.9.1+cpu)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn sentence-transformers faiss-cpu transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958f466",
   "metadata": {},
   "source": [
    "Instalasi Library dan Pra-pemrosesan Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b3181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dan Ekstraksi Metode Selesai. Data disimpan di: data/thesis_data_temp_c1.csv\n",
      "                                               judul methods_extracted\n",
      "0  Klasifikasi Musik Tradisional Menggunakan Meto...               svm\n",
      "1  Klasifikasi Musik Tradisional Menggunakan Meto...       transformer\n",
      "2  Clusterisasi Data Wisatawan Menggunakan Metode...               rnn\n",
      "3  Deteksi Serangan Siber Menggunakan Metode LSTM...              lstm\n",
      "4  Prediksi Cuaca Harian Menggunakan Metode RNN b...               rnn\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True) # Pastikan folder data ada\n",
    "\n",
    "# 1. Muat data aktual\n",
    "df = pd.read_csv(\"sample_skripsi_it_variatif_100.csv\")\n",
    "df['id'] = df.index # Tambahkan ID\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Normalisasi teks: lowercase, hapus non-alphanumeric, dan hapus spasi berlebih.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_methods_rule_based(text):\n",
    "    \"\"\"\n",
    "    Ekstraksi Metode: Menggunakan rule-based (regex) untuk mencari metode populer.\n",
    "    \"\"\"\n",
    "    methods_keywords = [\n",
    "        r'cnn', r'svm', r'k-means', r'rnn', r'lstm', r'transformer', \n",
    "        r'yolov8', r'a\\* search', r'agile', r'waterfall', r'scrumban', \n",
    "        r'naive bayes', r'deep learning', r'machine learning', r'aes'\n",
    "    ]\n",
    "    \n",
    "    extracted = []\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for method in methods_keywords:\n",
    "        # Cari metode sebagai kata terpisah\n",
    "        if re.search(r'\\b' + method + r'\\b', text_lower):\n",
    "            extracted.append(method.replace(r'\\b', '').replace(r'\\*', '*'))\n",
    "            \n",
    "    return \", \".join(sorted(list(set(extracted))))\n",
    "\n",
    "# Terapkan pra-pemrosesan dan ekstraksi\n",
    "df['abstract_clean'] = df['abstrak'].apply(preprocess_text)\n",
    "df['methods_extracted'] = df['abstrak'].apply(extract_methods_rule_based)\n",
    "df['title_clean'] = df['judul'].apply(preprocess_text)\n",
    "\n",
    "# *** LANGKAH PENTING: SIMPAN HASIL KE CSV ***\n",
    "TEMP_DATA_PATH = \"data/thesis_data_temp_c1.csv\"\n",
    "df.to_csv(TEMP_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"Preprocessing dan Ekstraksi Metode Selesai. Data disimpan di: {TEMP_DATA_PATH}\")\n",
    "print(df[['judul', 'methods_extracted']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad49ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muat data hasil Cell 1\n",
    "df = pd.read_csv(\"D:\\SistemChatbot\\sample_skripsi_it_variatif_100.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f38dd3",
   "metadata": {},
   "source": [
    "Semi-Otomatis Labeling Topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a825cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Otomatis Labeling Topik Selesai.\n",
      "                                               judul methods_extracted  \\\n",
      "0  Klasifikasi Musik Tradisional Menggunakan Meto...               svm   \n",
      "1  Klasifikasi Musik Tradisional Menggunakan Meto...       transformer   \n",
      "2  Clusterisasi Data Wisatawan Menggunakan Metode...               rnn   \n",
      "3  Deteksi Serangan Siber Menggunakan Metode LSTM...              lstm   \n",
      "4  Prediksi Cuaca Harian Menggunakan Metode RNN b...               rnn   \n",
      "5  Deteksi Emosi pada Teks Menggunakan Metode YOL...            yolov8   \n",
      "6  Deteksi Emosi pada Teks Menggunakan Metode Nai...       naive bayes   \n",
      "7  Pengenalan Wajah untuk Absensi Menggunakan Met...               svm   \n",
      "8  Deteksi Serangan Siber Menggunakan Metode LSTM...              lstm   \n",
      "9  Deteksi Emosi pada Teks Menggunakan Metode ARI...               NaN   \n",
      "\n",
      "   topic_label_id  \n",
      "0               0  \n",
      "1               0  \n",
      "2               2  \n",
      "3               0  \n",
      "4               0  \n",
      "5               0  \n",
      "6               0  \n",
      "7               4  \n",
      "8               0  \n",
      "9               0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# *** LANGKAH PENTING: MUAT KEMBALI DATA DARI FILE PERANTARA ***\n",
    "TEMP_DATA_PATH = \"data/thesis_data_temp_c1.csv\"\n",
    "df = pd.read_csv(TEMP_DATA_PATH)\n",
    "\n",
    "# 1. Definisi Topik dan Kata Kunci\n",
    "topic_mapping = {\n",
    "    0: {\"name\": \"Klasifikasi/Deteksi\", \"keywords\": [\"klasifikasi\", \"deteksi\", \"prediksi\", \"citra\", \"musik\", \"penyakit\"]},\n",
    "    1: {\"name\": \"Sistem Informasi/Aplikasi\", \"keywords\": [\"sistem informasi\", \"rancang bangun\", \"aplikasi\", \"akademik\", \"e-commerce\", \"pengembangan aplikasi\"]},\n",
    "    2: {\"name\": \"Clusterisasi/Rekomendasi\", \"keywords\": [\"clusterisasi\", \"rekomendasi\", \"wisatawan\", \"pelanggan\"]},\n",
    "    3: {\"name\": \"Jaringan/Siber\", \"keywords\": [\"jaringan\", \"siber\", \"kerentanan\", \"keamanan\", \"iot\", \"cloud\"]},\n",
    "}\n",
    "DEFAULT_LABEL_ID = 4\n",
    "topic_mapping[DEFAULT_LABEL_ID] = {\"name\": \"Lain-lain/Umum\", \"keywords\": []}\n",
    "\n",
    "\n",
    "def semi_automatic_labeling(title, abstract):\n",
    "    \"\"\"Memberikan label ID berdasarkan kata kunci pada judul dan abstrak.\"\"\"\n",
    "    # Pastikan menggunakan data yang sudah di-clean/lowercase untuk pencarian\n",
    "    text = (title + \" \" + abstract).lower() \n",
    "    \n",
    "    # Prioritaskan Topik 0-3\n",
    "    for label_id, data in topic_mapping.items():\n",
    "        if label_id == DEFAULT_LABEL_ID: continue\n",
    "            \n",
    "        for kw in data['keywords']:\n",
    "            if kw in text:\n",
    "                return label_id\n",
    "    \n",
    "    return DEFAULT_LABEL_ID\n",
    "\n",
    "# 2. Terapkan labeling\n",
    "df['topic_label_id'] = df.apply(lambda row: semi_automatic_labeling(row['title_clean'], row['abstract_clean']), axis=1)\n",
    "\n",
    "# 3. Simpan mapping label ke CSV\n",
    "topic_labels_df = pd.DataFrame([\n",
    "    {'topic_label_id': id, 'topic_name': data['name']} \n",
    "    for id, data in topic_mapping.items()\n",
    "])\n",
    "topic_labels_df.to_csv(\"topic_mapping.csv\", index=False)\n",
    "\n",
    "# 4. Simpan data yang sudah diperkaya fitur (FINAL FEATURE DATA)\n",
    "df.to_csv(\"thesis_data_features.csv\", index=False)\n",
    "\n",
    "print(\"Semi-Otomatis Labeling Topik Selesai.\")\n",
    "print(df[['judul', 'methods_extracted', 'topic_label_id']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6d46a",
   "metadata": {},
   "source": [
    "Membangun Indexing (Retrieval Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4868d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model SBERT berhasil dimuat dan disimpan di: models/sbert_model\n",
      "Membuat embeddings untuk 100 abstrak menggunakan cpu...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f4e744358e40978b45f371dc0fe583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings selesai. Dimensi vektor: 768\n",
      "Index FAISS berhasil dibangun (N=100) dan disimpan di: data/faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. Konfigurasi\n",
    "SBERT_MODEL_PATH = \"models/sbert_model\"\n",
    "FAISS_INDEX_PATH = \"data/faiss_index.bin\"\n",
    "os.makedirs(SBERT_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Muat data yang sudah dibersihkan\n",
    "df = pd.read_csv(\"thesis_data_features.csv\")\n",
    "texts = df['abstract_clean'].tolist()\n",
    "\n",
    "# 2. Muat dan Simpan Model SBERT\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') \n",
    "model.save(SBERT_MODEL_PATH)\n",
    "print(f\"Model SBERT berhasil dimuat dan disimpan di: {SBERT_MODEL_PATH}\")\n",
    "\n",
    "# 3. Buat Embeddings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Membuat embeddings untuk {len(texts)} abstrak menggunakan {device}...\")\n",
    "with torch.no_grad():\n",
    "    embeddings = model.encode(texts, \n",
    "                              convert_to_numpy=True, \n",
    "                              show_progress_bar=True,\n",
    "                              device=device)\n",
    "\n",
    "embeddings = np.asarray(embeddings, dtype=np.float32) \n",
    "D = embeddings.shape[1] \n",
    "print(f\"Embeddings selesai. Dimensi vektor: {D}\")\n",
    "\n",
    "# 4. Membangun dan Menyimpan Index FAISS\n",
    "index = faiss.IndexFlatL2(D)\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "\n",
    "print(f\"Index FAISS berhasil dibangun (N={index.ntotal}) dan disimpan di: {FAISS_INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15282dc3",
   "metadata": {},
   "source": [
    "Membangun Klasifikasi Topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab62b8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.12.12)\n",
      "Requirement already satisfied: anyio in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c2e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7f34aedd16462c9e92e4bbb89d3249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4af37242644095b3deb8fbfe6a36e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Fine-Tuning Klasifikasi Topik IndoBERT selesai.\n",
      "Semua konfigurasi data dan trainer sudah diperbaiki. Silakan jalankan trainer.train() di cell berikutnya.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_2384\\1123665902.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "\n",
    "# Muat data dengan label topik baru\n",
    "df = pd.read_csv(\"thesis_data_features.csv\")\n",
    "topic_labels_df = pd.read_csv(\"topic_mapping.csv\")\n",
    "\n",
    "# 1. Pembagian Data (Fix Imbalance/Single Class Error)\n",
    "X = df['abstract_clean'].tolist()\n",
    "y = df['topic_label_id'].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) \n",
    "\n",
    "# 2. Setup Model dan Tokenizer\n",
    "MODEL_NAME = \"indolem/indobert-base-uncased\"\n",
    "NUM_LABELS = topic_labels_df.shape[0] \n",
    "CLASSIFIER_MODEL_PATH = \"models/topic_classifier\"\n",
    "os.makedirs(CLASSIFIER_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "# --- PERBAIKAN FUNGSI TOKENISASI UTAMA ---\n",
    "def tokenize_and_add_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks dan secara eksplisit menambahkan kolom 'labels' ke output.\n",
    "    \"\"\"\n",
    "    tokenized_output = tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "    tokenized_output['labels'] = examples['labels'] \n",
    "    return tokenized_output\n",
    "# ----------------------------------------\n",
    "\n",
    "# 3. Konversi ke Hugging Face Dataset (PERBAIKAN UTAMA DI SINI)\n",
    "train_df = pd.DataFrame({'text': X_train, 'labels': y_train}) \n",
    "val_df = pd.DataFrame({'text': X_val, 'labels': y_val})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(\n",
    "    tokenize_and_add_labels, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\"] # HANYA HAPUS 'text'\n",
    ")\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_df).map(\n",
    "    tokenize_and_add_labels, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\"] # HANYA HAPUS 'text'\n",
    ")\n",
    "# ----------------------------------------\n",
    "\n",
    "# 4. Definisikan Metrik Evaluasi\n",
    "def compute_metrics(p):\n",
    "    logits = p.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    f1_macro = f1_score(p.label_ids, predictions, average='macro', zero_division=0)\n",
    "    acc = accuracy_score(p.label_ids, predictions)\n",
    "    return {'accuracy': acc, 'f1_macro': f1_macro}\n",
    "\n",
    "# 5. Definisikan Hyperparameters Training (Minimal untuk menghindari TypeErrors)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CLASSIFIER_MODEL_PATH,\n",
    "    num_train_epochs=5,                          \n",
    "    per_device_train_batch_size=8,             \n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,                        \n",
    "    logging_steps=100,\n",
    "    do_eval=True \n",
    ")\n",
    "\n",
    "# 6. Setup Trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Setup Fine-Tuning Klasifikasi Topik IndoBERT selesai.\")\n",
    "print(\"Semua konfigurasi data dan trainer sudah diperbaiki. Silakan jalankan trainer.train() di cell berikutnya.\")\n",
    "\n",
    "# Setelah pelatihan, Anda dapat melanjutkan ke Tahap 5 (Generation Setup)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5cd3b3",
   "metadata": {},
   "source": [
    "Title Suggestion / Generation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e817a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad241468a7da4563bf1d750f6448b23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\acer\\.cache\\huggingface\\hub\\models--google--mt5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311fd9c88ae14fe38ccd0ed74fc599fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69954ce7f564f8a97d027bf46e8eb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Column to remove ['__index_level_0__'] not in the dataset. Current columns in the dataset: ['input_text', 'target_text']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m train_df = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33minput_text\u001b[39m\u001b[33m'\u001b[39m: X_train, \u001b[33m'\u001b[39m\u001b[33mtarget_text\u001b[39m\u001b[33m'\u001b[39m: y_train})\n\u001b[32m     38\u001b[39m val_df = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33minput_text\u001b[39m\u001b[33m'\u001b[39m: X_val, \u001b[33m'\u001b[39m\u001b[33mtarget_text\u001b[39m\u001b[33m'\u001b[39m: y_val})\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m train_dataset = \u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__index_level_0__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m val_dataset = Dataset.from_pandas(val_df).map(\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m examples: preprocess_function(examples, tokenizer), \n\u001b[32m     48\u001b[39m     batched=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m     49\u001b[39m     remove_columns=[\u001b[33m\"\u001b[39m\u001b[33minput_text\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtarget_text\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m__index_level_0__\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     50\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 5. Definisikan Hyperparameters Training (Minimal)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3109\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3107\u001b[39m     missing_columns = \u001b[38;5;28mset\u001b[39m(remove_columns) - \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m._data.column_names)\n\u001b[32m   3108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[32m-> \u001b[39m\u001b[32m3109\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3110\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn to remove \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._data.column_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   3111\u001b[39m         )\n\u001b[32m   3113\u001b[39m load_from_cache_file = load_from_cache_file \u001b[38;5;28;01mif\u001b[39;00m load_from_cache_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_caching_enabled()\n\u001b[32m   3115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Column to remove ['__index_level_0__'] not in the dataset. Current columns in the dataset: ['input_text', 'target_text']"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Muat data yang sudah dibersihkan\n",
    "df = pd.read_csv(\"thesis_data_features.csv\")\n",
    "\n",
    "# 1. Pembagian Data (Train/Val)\n",
    "X = df['abstract_clean'].tolist()\n",
    "y = df['title_clean'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Setup Model dan Tokenizer\n",
    "MODEL_NAME = \"google/mt5-small\" \n",
    "GENERATOR_MODEL_PATH = \"models/title_generator\"\n",
    "os.makedirs(GENERATOR_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# Tokenizer mT5 memerlukan SentencePiece\n",
    "# Jika error SentencePiece masih muncul, coba tambahkan use_fast=False (meskipun tidak perlu setelah SentencePiece terinstal)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 3. Fungsi Tokenisasi untuk T5\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [f\"generate title: {abstract}\" for abstract in examples['input_text']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    labels = tokenizer(examples['target_text'], max_length=64, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 4. Konversi ke Hugging Face Dataset\n",
    "train_df = pd.DataFrame({'input_text': X_train, 'target_text': y_train})\n",
    "val_df = pd.DataFrame({'input_text': X_val, 'target_text': y_val})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(\n",
    "    lambda examples: preprocess_function(examples, tokenizer), \n",
    "    batched=True, \n",
    "    remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"] \n",
    ")\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_df).map(\n",
    "    lambda examples: preprocess_function(examples, tokenizer), \n",
    "    batched=True, \n",
    "    remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "# 5. Definisikan Hyperparameters Training (Minimal)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=GENERATOR_MODEL_PATH,\n",
    "    num_train_epochs=5,                       \n",
    "    per_device_train_batch_size=4,          \n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-4,                       \n",
    "    logging_steps=100,\n",
    "    do_eval=True,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "# 6. Setup Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Setup Fine-Tuning Title Generation (T5) Selesai.\")\n",
    "print(\"Sekarang Anda dapat menjalankan trainer.train() di cell berikutnya untuk memulai pelatihan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893d19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.5/1.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 2.4 MB/s  0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ef9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 04:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pelatihan Title Generation Selesai. Model disimpan di: models/title_generator\n"
     ]
    }
   ],
   "source": [
    "# Pastikan Anda telah menjalankan semua kode setup Tahap 5\n",
    "# dan objek 'trainer', 'tokenizer', dan 'CLASSIFIER_MODEL_PATH' sudah terdefinisi.\n",
    "\n",
    "# Mulai pelatihan\n",
    "trainer.train()\n",
    "\n",
    "# Simpan model terbaik setelah pelatihan selesai\n",
    "trainer.save_model(GENERATOR_MODEL_PATH)\n",
    "tokenizer.save_pretrained(GENERATOR_MODEL_PATH)\n",
    "\n",
    "print(f\"Pelatihan Title Generation Selesai. Model disimpan di: {GENERATOR_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
